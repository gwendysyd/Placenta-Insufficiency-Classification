# **Experiment Plan: Applying Different Architectures to Placenta Ultrasound Images Datasets**

## **Purpose**
This experiment aims to systematically compare the performance of various neural network architectures on the same image dataset. By employing both basic and advanced models with different architecture sizes, with and without transfer learning, we aim to identify the most effective approaches for image analysis in this context.

## **Experimental Approach**
We will apply different neural network architectures to the datasets in their original form to assess the effectiveness of each model and to get a baseline. We use image augmentation, and test the approach with and without normalized class weights. We take original mask size. In this iteration, we are opting for a 'softer' image augmentation approach.

## **Datasets for Analysis**
- **C3 all planes**: At least one of three conditions (CIR, preeclampsia, birth weight percentile <10) is present.
- **LBW all planes**: Focus on birth weight percentile <10 only.
- **CIR all planes**: Focus on CIR only.
- **PRE all planes**: Focus on preeclampsia only.

## **Key parameters**
- **Image augmentation**: Applied.
- **Class weights**: Tested with and without class weights.
- **Placental planes**: Anterior and posterior combined.
- **Mask size**: 1:1.

## **Neural Network Architectures**
- **Basic Architectures**:
  - Basic ANN: A simple artificial neural network as a baseline.
- **VGG16 Variants**:
  - VGG16, initializing with ImageNet weights, but all weights are updated.
  - VGG16, with transfer learning (ImageNet weights are frozen). The base model is frozen to prevent overfitting, given that our dataset is small. We want to leverage the features learned from ImageNet effectively. The custom top layers have been kept at a minimum and as simple as possible to prevent overfitting.
- **ResNet50 Variants**:
  - ResNet50, initializing with ImageNet weights, but all weights are updated.
  - ResNet50, with transfer learning (ImageNet weights are frozen). The base model is frozen to prevent overfitting, given that our dataset is small. We want to leverage the features learned from ImageNet effectively. The custom top layers have been kept at a minimum and as simple as possible to prevent overfitting.
- **MobileNet Variants**:
  - MobileNet, initializing with ImageNet weights, but all weights are updated.
  - MobileNet, with transfer learning (ImageNet weights are frozen). The base model is frozen to prevent overfitting, given that our dataset is small. We want to leverage the features learned from ImageNet effectively. The custom top layers have been kept at a minimum and as simple as possible to prevent overfitting.
- **ResNet18 Variants**:
  - ResNet18, initializing with ImageNet weights, but all weights are updated.
  - ResNet18, with transfer learning (ImageNet weights are frozen). The base model is frozen to prevent overfitting, given that our dataset is small. We want to leverage the features learned from ImageNet effectively. The custom top layers have been kept at a minimum and as simple as possible to prevent overfitting.

